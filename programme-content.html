<div style="background-color: rgba(151,211,252,0.23); padding: 10px">
        <div class="ui list">
            <div class="item">
                <div class="content">
                   <div class="header">The final program will be provided in May</div>
                </div>
            </div>
        </div>
  </div>


       <!-- <h3 class="ui large header">
           
            <div class="content">
                <strong>Final Program</strong>
            </div>
        </h3>
 <div class="header">The final program is available <a href="program_iwsg2023.pdf">here</a></div> -->

<!-- <script>
document.cookie = "utcOffset = "; // clear cookie
var timezone_offset_hours = new Date().getTimezoneOffset();
timezone_offset_hours = (timezone_offset_hours == 0 ? 0 : -timezone_offset_hours)/60;

// Timezone difference in minutes such as 330 or -360 or 0
console.log(timezone_offset_hours);
document.cookie = "utcOffset = " + timezone_offset_hours;
</script>-->

  <!--<div style="background-color: rgba(151,211,252,0.23); padding: 10px">
        <div class="ui list">
            <div class="item">
                <div class="content">
                   <div class="header">The final program will be provided in May</div>
                </div>
            </div>
        </div>
  </div>
<br>-->



<!--<div class="ui large header">
    Workshop Programme<br>
    <br>
    <div class="sub header" style="font-size: 0.6em">
        <a href="https://notredame.zoom.us/j/93215123088" target="_blank">
            <button class="ui mini orange button">
                <i class="map marker alternate icon"></i>
                Zoom Meeting Link
            </button>
        </a>
    </div>
</div>-->

<!-- 
<table class="ui single line mini table">
    <thead>
    <tr>
        <th class="two wide">Time</th>
        <th>Event</th>
        <th></th>
    </tr>
    </thead>
    <tbody>

    <tr style="background-color: rgba(0,0,255,0.23)">
                <td> <div class="header">DAY ONE</div></td>
                <td>
                    <div class="ui list">
                        <div class="item">
                            <div class="content">
                               Thursday, June 10th 2021
                            </div>
                        </div>
                    </div>
                </td>
                <td></td>
            </tr>

    <tr>
        <td>1:00&mdash;1:15</td>
        <td>
            <div class="ui list">
                <div class="item">
                    <div class="header">Welcome</div>
                    Sandra Gesing, Iain Barclay, Ian Taylor
                </div>
            </div>
        </td>
    </tr>
    <tr style="background-color: rgba(151,211,252,0.23)">
        <td>1:15&mdash;1:45</td>
        <td>
            <div class="ui list">
                <div class="item">
                    <div class="header">Keynote: Alun Preece</div>
                    <div class="ui accordion">
                        <div class="title">
                            <i class="dropdown icon"></i>Explainable AI at the Edge: Perspectives & Requirements there used to be an </a> tag here, which didn't seem right - although removing didn't fix the problem with accordion not working
                        </div>
                        <div class="content">
                            <p class="transition hidden">
                           Getting modern artificial intelligence (AI) and machine learning (ML) services into the hands of front-line users requires addressing what’s being called the “last mile AI” challenge. This means translating the benefits of AI from lab to the field, moving from often idealised ML model training to messy systems settings involving humans and AI software needing to work effectively together. Rapid trust calibration is a key part of this challenge — how to enable front-line users to understand the capabilities and limitations of complex AI services to exploit their benefits while mitigating their weaknesses. In this talk I’ll look an explainable AI from the “last mile” perspective, considering in particular settings at or near the network edge, including mobile and field deployments. I’ll consider multiple perspectives including the explanation and traceability requirements of different kinds of system stakeholders, human/machine teaming, and the interconnected nature of explainability, trust, assurance and scrutiny.
<br><br>
                              Prof Alun Preece is Co-Director of Cardiff University's Crime & Security Research Institute and Deputy Head of the School of Computer Science and Informatics. He is the UK Academic Technical Area Lead for the US/UK Distributed Analytics and Information Sciences International Technology Alliance (DAIS ITA, 2016-2026) funded by the US & UK Governments and led by IBM, in which he also leads research in Anticipatory Situational Understanding involving a team from Airbus, BAE Systems, IBM, UCL, and UCLA. Previously, he served in the same role for the Network and Information Sciences International Technology Alliance (NIS ITA, 2006-2016).
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </td>
    </tr>
    <tr class="active">
        <td colspan="3"><strong>Data Concepts</strong></td>
    </tr>


    <tr>
        <td>1:45&mdash;2:00</td>
        <td>
            <div class="ui list">
                <div class="item">
                    <div class="header">The DataPLANT Hub - A Science Gateway for the Fundamental Plant Research Community</div>
                    Jens Krüger
                    <div class="ui accordion">
                    <div class="title">
                        <i class="dropdown icon"></i>Abstract
                    </div>
                    <div class="content">
                    <p class="transition hidden">
                        To facilitate the collection, processing, exchange and archiving of research data scientists increasingly rely on research data management services. DataPLANT (https://nfdi4plants.de/) generates added value in the field of fundamental plant research as modern, integrated science gateway, improving the reproducibility of research, the linkage of expertise across disciplines, the sharing of research data and the collection of metadata. The project is part of the newly founded national research data infrastructure in Germany. 
It is anticipated to create a service environment which facilitates the contextualization of research data according to the FAIR principles. While keeping the effort for annotation at a minimal level, the whole data live cycle in fundamental plant research shall be accompanied. To achieve this ambitious goal DataPLANT relies on technical-digital assistance and personal at-site support. 
DataPLANT can rely on exhaustive technical infrastructures for compute and storage, as the de.NBI Cloud in Tübingen and Freiburg, the bwCloud, the bwForCluster BinAC, EOSC-Life, the bwSFS in Tübingen and Freiburg, as well as further distributed resources. Hence, broad infrastructural capacities can be offered to the plant research community. 
Wherever possible, DataPLANT relies on a single-sign-on concept, building on the ELIXIR AAI. Through the usage of ELIXIR IDs in conjunction with ORCID as persistent identifier, a unique and resilient association of users with their research data is ensured. Keycloak is used as authorisation service enabling user access to the DataPLANT Hub after authentication against the identity provider instance of their home organization. In conjunction with the functionalities of the DataPLANT Hub which builds on Hubzero and the underlying infrastructure a fine granular user and role management is enabled. Users will experience a simple and straight authentication process enabling access to the DataPLANT Hub and all services directly offered through it. Further compute, storage and workflow services, as well as remote repositories and archives may be connected through this mechanism. Consequently users are relieved from the need to manually orchestrate multiple identities, passwords and other credentials. 
The development within the DataPLANT consortium largely benefit from synergies with the BioDATEN science data center, using individual realms within the same Keycloak instance. BioDATEN relies on similar technologies, especially with respect to authentication and authorization mechanisms. 
Through this and further implementation, an extra value is provided for the fundamental plant research community. 
                    </p>
                    </div>
                </div>
            </div>
        </td>
    </tr>

    <tr>
      <td>2:00&mdash;2:15</td>
      <td>
            <div class="ui list">
                <div class="item">
                    <div class="header">GEnerator Of GEospatial dAta pRoducts (GEOGEAR): An interactive tool for generating integrated geospatial data products from distributed sources</div>
                    Tim Lenters, Zhiming Zhao and Daniel Kissling
                    <div class="ui accordion">
                    <div class="title">
                        <i class="dropdown icon"></i>Abstract
                    </div>
                    <div class="content">
                    <p class="transition hidden">
                        During the last decades, there has been a large increase in data availability within different fields of ecology and biogeography. This follows a similar trend as the general increase in data volumes in other disciplines in the past decade. Large data volumes are an important part of ‘big data’, and one of the four Vs, with the other three being: velocity (rapidly changing), variety (diverse data sources) and veracity (large uncertainties). The aggregation of these available data from a variety of sources could provide research opportunities which were unfeasible several years ago, especially at a macroecological scale. Both biotic and abiotic datasets have been aggregated in ecological research in recent years. Examples are: combining biodiversity threat layers and species distribution maps for conservation analyses, integrating climate and geology datasets for global species richness analyses and aggregating different topographic variables to be used for environmental and biodiversity modelling. 
Among those examples, integration of diverse data products is crucial. There are existing methods (e.g. as used in the abovementioned examples) and tools (e.g. from R-packages, GIS software) which support scientists in making such integrated data products, but these tools are only useable for specific purposes, and the efficiency for processing multilayer data products (i.e. when the resulting data product consists of multiple layers) is very low. An interactive tool that would allow this is needed. However, the implementation of such an integrated tool faces several challenges. First, the scientific need for integrated data products is diverse. They require data from different sources which are often provided by different organizations and use different metadata standards. Second, the raw data involved for integrating data products are often in large volume and geospatial layers from those data consist of many different types. The computation for integrating data products is often resource demanding and as a consequence, the processing of those datasets to meet specific application needs (e.g. deadlines) is difficult. Third, an integrated data product is often generated via customized workflows executed across different machines. The reproducibility of such data products requires rich contextual information. 
To address these challenges, we present an interactive spatial analysis tool (GEnerator Of GEospatial dAta pRoducts; GEOGEAR). With this tool, we tackle the beforementioned challenges: First, GEOGEAR provides an user-friendly graphical interface to support interactive selection of data sources, layers and parameters. Second, the tool can automatically generate workflows based on the selected input and schedule the parallelized execution on a remote cloud infrastructure. Third, during the workflow execution the tool collects the contextual information of the evolution of the data products using the standardized provenance schema: PROV-O . Finally, the tool also creates the metadata information (using ISO-19115 ) of the data products, and allows the user to publish the data products in suitable community catalogues (e.g. LifeWatch catalogue). In this way, the FAIRness of the data product can be realized. 
The tool will be open-source. The service of GEOGEAR will be operated as part of the virtual research environment developed by LifeWatch-VLIC. 
                    </div>
                </div>
            </div>
        </td>
    </tr>
    <tr>
      <td>2:15&mdash;2:30</td>
      <td>
            <div class="ui list">
                <div class="item">
                    <div class="header">Health Data Research Innovation Gateway — A gateway to discover, explore, access and analyze health research data in the United Kingdom</div>
                    Susheel Varma, Peggy Barthes-Streit, Vicky Hellon, Gerry Reilly and Charles Gibbons
                    <div class="ui accordion">
                    <div class="title">
                    <i class="dropdown icon"></i>Abstract
                    </div>
                    <div class="content">
                    <p class="transition hidden">
                     Data has been widely hailed as ‘the new raw material of the 21st century' and the better use & interoperability of health data is a foundational component of the vision set out for the future of healthcare in the United Kingdom. With funding provided by the Industrial Strategy Challenge Fund, Health Data Research UK (https://www.hdruk.ac.uk/) has built the Health Data Research Innovation Gateway (https://www.healthdatagateway.org/) as an interoperable access point for metadata of health data resources in the United Kingdom. The Gateway is an integral part of HDR UK’s strategy to bring together the UK’s health data custodians via UK Health Data Research Alliance (https://ukhealthdata.org/), data quality/utility improvements via Health Data Research Hubs (https://www.hdruk.ac.uk/help-with-your-data/our-hubs-across-the-uk/) for national health data priority areas – Understanding the cause of disease, Clinical Trials, Better Care and Public Health. Launched in early 2020, the number of health data resources on the Gateway has steadily risen to over 1,900 health data assets, including 616 datasets, 150 tools, 195 educational courses and 936 publications that cover a significant breadth and depth of the health data research landscape in a relatively short period (~6 months). The Gateway is not limited to being a metadata registry, but forms an entire ecosystem for fostering the curation, quality improvement, access and ethical use of health data accessible by over 1000 registered health data researchers in the United Kingdom. 

The design and implementation of the Gateway posed a number of challenges: (1) using a design-led approach for the development of user experience for multiple stakeholders – data custodians, researchers, developers, funders and the patient/public; (2) the development of metadata standards and APIs aligned to existing international standards; (3) integration of external sources of metadata beyond manual curation of resources; (4) development of community-led metadata completeness, compliance and utility improvement framework; (5) streamlining and harmonisation of a single data access request process that provided maximum coverage of datasets on the Gateway; (6) coordination of advanced/federated analytics capabilities to improve the relevancy and utility of the datasets; and (7) support and management of communities around health data resources. 

Our design-thinking approach for the Gateway has, and continues to be, heavily influenced by frequent stakeholder and user workshops that have resulted in the development of an end-to-end user journey for all of our stakeholders. This stakeholder engagement has actively engaged with the Patient, Public and Practitioners communities to ensure that there is constant public and patient input into all aspects of the Gateway development. The high-level phases of a typical research users’ journey on the Gateway are described in the sections below, namely Discover, Access, Analyse and Building Communities (Figure 1). 

In this talk, we present our current approach, implementation, our lessons learned from the development of the Gateway infrastructure and present the future directions and federated challenges for the development of the Gateway.
                    </div>
                </div>
            </div>
        </td>
    </tr>

        <tr class="warning">
            <td>2:30&mdash;3:00</td>
            <td>
                <div class="ui list">
                    <div class="item">
                        <i class="coffee icon"></i>
                        <div class="content">
                            <div class="header">Tea / Coffee Break</div>
                        </div>
                    </div>
                </div>
            </td>
            <td></td>
        </tr>

    <tr style="background-color: rgba(151,211,252,0.23)">
        <td>3:00&mdash;3:30</td>
        <td>
            <div class="ui list">
                <div class="item">
                    <div class="header">Keynote: Jarek Nabrzyski</div>
                    <div class="ui accordion">
                        <div class="title">
                            <i class="dropdown icon"></i>Adapting Distributed Ledger Technology for Open Science
                        </div>
                        <div class="content">
                            <p class="transition hidden">
                            Distributed Ledger Technology (DLT), including blockchain
technology,  has gained lots of interest in various domains of our
lives. On top of all chatter about DLT, there are some misconceptions
being passed around it. Its value and potential applications are often
undermined, especially when it is confused with Bitcoin. In this talk,
Jarek will discuss various use cases from academia where DLT can have
a substantial,   enabling role in scientific discovery. But can DLT
provide transformational capabilities as scientists strive to address
grand challenges in science and engineering?<br/><br/>
                            Jarek Nabrzyski is the founding director of the Center for Research Computing and a concurrent professor in the Department of Computer Science and Engineering at the University of Notre Dame. Since 2016 Nabrzyski has been also overseeing the operations of the Center for Social Science Research at Notre Dame. His research is focused on resource management in distributed and exascale computing systems as well as blockchain distributed applications. He enjoys building and managing teams and engaging in complex science and industry collaborations.  Before joining Notre Dame in 2009, Nabrzyski was the executive director of the Center for Computation and Technology at the Louisiana State University, and, before that, he was the Scientific Applications Department manager at Poznan Supercomputing and Networking Center.  Nabrzyski is involved in several Notre Dame startups, such as SimbaChain, Sciath FinTech (founding member) and Digital Leader Academy, Delive, and FloWaste (advisor).
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </td>
    </tr>
    <tr class="active">
        <td colspan="3"><strong>The Design and Operation of Science Gateways</strong></td>
    </tr>

    <tr>
        <td>3:30&mdash;3:45</td>
        <td>
            <div class="ui list">
                <div class="item">
                    <div class="header">FAIR Research Software and Science Gateways</div>
                    Daniel S. Katz, Michelle Barker, Leyla Jael Castro, Neil P. Chue Hong, Morane Gruenpeter, Jen Harrow, Carlos Martinez, Paula Andrea Martinez and Fotis Psomopoulos
                    <div class="ui accordion">
                    <div class="title">
                    <i class="dropdown icon"></i>Abstract
                    </div>
                    <div class="content">
                    <p class="transition hidden">
                    In recent years, the scholarly community has examined its culture and practices, and found a set of overlapping areas in which to improve, including open science (making both the outputs and processes of scholarly research available), reproducibility (increasing trust in scholarly results by making them repeatable by others), and FAIR (making scholarly outputs, specifically data, findable, accessible, interoperable, and reusable). While the scholarly community is generally supportive of all of these efforts, the degree of support wanes with both the amount of extra work that is needed and the lack of clear details on how to achieve them, along with misaligned incentives. 

In this lightning talk, we will initially focus on FAIR and the details of how it can be applied to research software. This leads to a number of distinct challenges, including scope (defining both software and research software), principles (defining what findable, accessible, interoperable, and reusable mean for research software), implementation (developing guidelines and instructions for how to make research software FAIR), and metrics (providing a means to measure the FAIRness of research software). 

Science gateways include a number of different types of software, for example, the frameworks used to construct the gateways themselves, tools provided by the community that run in the gateway, and software implemented as services with which the gateways interact. The second part of this lightning talk will discuss how FAIR principles for research software apply to each of these types of software common in science gateways. 

We will close by explaining how members of the science gateway community can become more involved in the FAIR for research software process, to learn, to contribute, or to champion.
                      </div>                
              </div>
            </div>
        </td>
    </tr>

    <tr>
        <td>3:45&mdash;4:00</td>
        <td>
            <div class="ui list">
                <div class="item">
                  <div class="header">PITHIA-NRF e-Science Centre – A Science Gateway based on Cloud-based Reference Architectures</div>
                    Gabriele Pierantoni, Tamas Kiss and Alexander Bolotov.
                    <div class="ui accordion">
                    <div class="title">
                    <i class="dropdown icon"></i>Abstract
                    </div>
                    <div class="content">
                    <p class="transition hidden">
                   Plasma instabilities, electromagnetic coupling and thermospheric wind-dynamics are the main physical mechanisms that lead to a coupled system that is composed of the Earth’s Ionosphere, Thermosphere, and Plasmasphere. These complex physical processes cause the variability of main physical characteristics in long and short terms. While we understand the broad features of this coupled system, we lack the depth of understanding of its variability that would allow scientists to build models with real predictive power. The PITHIA Network of Research Facilities (PITHIA-NRF) project, funded by the European Commission’s H2020 programme, aims at building a distributed network, integrating observing facilities, data collections, data processing tools and prediction models dedicated to ionosphere, thermosphere and plasmasphere research. 
One of the core components of PITHIA-NRF is a science gateway, the PITHIA-NRF e-Science Centre, that supports the execution of various scientific applications on distributed computing infrastructures. When designing and implementing the gateway, we follow a novel approach, based on the dynamic creation and instantiation of cloud-based reference architectures. Reference architectures are composed of multiple application components or microservices, described in the form of a deployment descriptor, and can be automatically deployed and managed at run-time. A reference architecture can include various components, such as generic or custom GUIs, data analytics, machine learning, simulation or other scientific applications, databases, and any other components (application-level firewalls, data converters, load balancers, etc.) that are required to realise a particular user scenario. 
Based on reference architectures, a generic a science gateway framework is currently being developed, as illustrated in Figure 1. The proposed gateway framework has two conceptually different major building blocks: the e-Science Centre and the various Reference Architectures. The e-Science Centre is a centrally deployed and maintained component that provides user management services, e-learning support, and the capability to store, search, compose and launch reference architectures. Reference Architectures (RA), on the other hand, are dynamically created and managed infrastructures that are launched or destroyed on demand by a suitable cloud orchestrator. 
The implementation of the PITHIA-NRF e-Science Centre is currently ongoing. When implementing the solution, we reuse and customise several existing components, developed in previous projects. MiCADO is utilised as a cloud orchestrator and is responsible for deploying the reference architectures and managing their life-cycle based on user-defined policies. Each reference architecture has its own MiCADO orchestrator that is launched by the MiCADO (Reference Architecture) Launcher. As User Management and Reference Architecture Repository components we are customising the EMGUM (emGORA User Management) and EMGREPO (emGORA Repository of Executable Artefacts) components of the CloudiFacturing Platform, implemented within the EU CloudiFacturing project. SMARTEST, a knowledge repository that assists and facilitates learning by representing knowledge and learning activities as graphs, is utilised as Knowledge Repository Exchange and Learning and provides embedded e-learning support. The Reference Architecture Composer and the e-Science Center GUI are new components and will be developed in PITHIA-NRF. However, similar concepts, the Digital Marketplace of CloudiFacturing and the reference architecture composition solutions currently developed in the EU funded DIGITbrain project, will be reused.          
                      </div>                
                </div>
            </div>
        </td>
    </tr>

    <tr>
        <td>4:00&mdash;4:15</td>
        <td>
            <div class="ui list">
                <div class="item">
                   <div class="header">The HUBzero® Migration: An Overview</div>
                    Erich Huebner, Pascal Meunier, Sandra Gesing, Michael Zentner, Claire Stirm, Steven Clark, Nathan Snodgrass and Richard Wellner.
                    <div class="ui accordion">
                    <div class="title">
                    <i class="dropdown icon"></i>Abstract
                    </div>
                    <div class="content">
                    <p class="transition hidden">
                      The HUBzero® Platform was designed and deployed to support the research and educational community and originated in the form of nanoHUB around the nanotechnology community. The HUBzero® Platform was formed at Purdue University in 2007 and has since evolved to support 25+ research projects from various disciplines. In 2019, the platform and team moved to a new institution. Lasting from 2019-2020, the HUBzero® team migrated hardware, software, and the team to the San Diego Supercomputer Center (SDSC) at the University of California, San Diego. This included 25 supported research projects from a Purdue-based infrastructure and to new infrastructure at SDSC. Migrating the 25 supported projects from one institute to another with minimal disruption to the research communities was a logistical challenge. While such a big move is rare in a product’s life cycle, many lessons were learned, ranging from the multitude of changes needed to migrate to a new operating system to a VM hosting solution instead of hardware hosts. Implementation of new monitoring systems and creating new policies and procedures to manage the new hosts were also completed while collaborating with new teams at SDSC. 

With over 100 instances and 65 TBs of client data, the technical transfer was minimal in the overall migration. However, the most challenging and most time-consuming part of the migration was in coordinating the transfer. Each set of new VMs at SDSC were configured before the scheduled migration day, along with the bulk of the data pre-transferred. Together, the HUBzero® team coordinated with each hub team and the new infrastructure teams at SDSC to choose and plan a migration day for each hub and their community. On migration day, final data sync from the Purdue host to the new SDSC VMs was completed, followed by final Ansible playbooks. Finally, some manual testing was completed to ensure that the hub was fully functional, and all data was transferred. 

The move to SDSC was not solely moving hubs from one university to another. The team also improved the HUBzero® architecture to take advantage of services used at SDSC and update some existing processes. Before the move, the HUBzero® Platform used Ogre, a Purdue University host management solution, to manage our hosts - we now use Ansible at SDSC. We also migrated from OpenVZ for simulation tool session containers in favor of Docker, another industry standard. 

While many more changes can be discussed on how to migrate a science gateway to a new home effectively, it would not have been possible without the support of the 25 hub teams. The HUBzero® roadmap for the next year is to continue the immersion into the SDSC team and improve the adopted technologies listed above. 
                    </div>                
                </div>
            </div>
        </td>
    </tr>


    <tr style="background-color: rgba(251,111,152,0.23)">
             <td>4:15&mdash;4:30</td>
             <td>
                 <div class="ui list">
                     <div class="item">
                         <i class="cocktail icon"></i>
                         <div class="content">
                             <div class="header">Wrap Up, including preview of IWSG 2022!</div>
                         </div>
                     </div>
                 </div>
             </td>
             <td></td>
         </tr>

   </tbody>
</table>
-->


<!-- DAY TWO !!!!-->
<!--
<table class="ui single line mini table">
    <thead>
    <tr>
        <th class="two wide">Time</th>
        <th>Event</th>
        <th></th>
    </tr>
    </thead>
    <tbody>

   <tr style="background-color: rgba(0,0,255,0.23)">
                <td> <div class="header">DAY TWO</div></td>
                <td>
                    <div class="ui list">
                        <div class="item">
                            <div class="content">
                               Friday, June 11th 2020
                            </div>
                        </div>
                    </div>
                </td>
                <td></td>
            </tr>

    <tr style="background-color: rgba(151,211,252,0.23)">
        <td>1:30&mdash;2:00</td>
        <td>
            <div class="ui list">
                <div class="item">
                    <div class="header">Keynote: Tamas Kiss</div>
                    <div class="ui accordion">
                        <div class="title">
                            <i class="dropdown icon"></i>Application-level Orchestration in the Cloud to Edge Continuum for Science Gateways and Beyond 
                        </div>
                        <div class="content">
                            <p class="transition hidden">
                            Science Gateways went through significant changes during their over twenty years of history. While the first science gateways were typically supporting grid computing infrastructures, the emphasis in the last decade shifted increasingly towards clouds or even computational power closer to the edges of the network. One of the major challenges within this increasingly complex distributed computing landscape in the automated orchestration of applications. With the emergence of container technologies, applications are typically composed of a number of microservices and deployed in an automated and orchestrated way that enables seamless communication between the various components. Additionally, the behaviour of the application during its execution also needs to be monitored and changes in its deployment may be necessary due to resource scalability requirements, security constraints or the need to offload or migrate microservices between the various computational layers. MiCADO is an open-source application-level orchestrator that targets these challenges and provides a comprehensive solution for applications running in the edge to cloud computing continuum. This presentation gives an overview of challenges and possible solutions for the edge to cloud orchestration problem and illustrates how MiCADO solves these challenges, serving various user communities and science and industry gateways. 
                            <br/><br/>
                            Tamas Kiss is a Professor of Distributed Computing at the School of Computer Science and Engineering, Director of the Research Centre for Parallel Computing at the University of Westminster, and Editor in Chief of the Journal of Grid Computing published by Springer Nature. He has attracted over £30 Million research funding and has been leading national and European research projects related to scientific and enterprise applications of cloud computing technologies. He has been involved in the science gateway research community for over a decade and was a main contributor to the WS-PGRADE Science Gateway Framework and its related technologies. Ha has also chaired three previous IWSG workshops. Between 2013 and 2016 he acted as project director and scientific coordinator for the European CloudSME (Cloud-based Simulation Platform for Manufacturing and Engineering) project that developed a Cloud-based simulation solution for manufacturing and engineering SMEs. He is Chair of the Scientific Advisory Board of CloudSME UG, a start-up that has been established by ten European stakeholders to exploit the results of the CloudSME project. Recently he led the EU H2020 COLA (Cloud Orchestration at the Level of Application) project that investigated how a generic and pluggable framework that supports the optimal and secure deployment and run-time orchestration of cloud applications can be created. Currently, he is  a member of the Executive Board and Coordinator of Application Experiments in the EU H2020 CloudiFacturing (Cloudification of Production Engineering for Predictive Digital Manufacturing) project that brings and progresses advanced information and communications technology in the field of cloud-based modelling and simulation, data analytics for online factory data, and real-time support to European manufacturing SMEs. Since December 2018 he is also Project Coordinator for the EU H2020 ASCLEPIOS project that focuses on secure and scalable cloud applications in healthcare.

                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </td>
    </tr>
    <tr class="active">
        <td colspan="3"><strong>Dynamic Research Environments</strong></td>
    </tr>

        <tr>
            <td>2:00&mdash;2:15</td>
            <td>
                <div class="ui list">
                    <div class="item">
                        <div class="header">Dynamic Composition and Automated Deployment of Digital Twins for Manufacturing</div>
                        James Deslauriers, Tamas Kiss and József Kovács
                    <div class="ui accordion">
                    <div class="title">
                    <i class="dropdown icon"></i>Abstract
                    </div>
                    <div class="content">
                    <p class="transition hidden">
                      
                      Digital twins represent an emerging trend and are already widely applied in the manufacturing sector to simulate the behaviour of manufacturing lines or industrial products, and to enhance or optimize their performance. The DIGITbrain project, funded by the European Commission’s H2020 Programme aims to extend the traditional digital twin concept towards the Digital Product Brain that steers the behaviour and performance of an industrial product by coalescing its physical and digital dimensions, and by memorising the occurred (physical and digital) events throughout its entire lifecycle. With such capabilities, the Digital Product Brain can steer the quick and convenient customization and repurposing of manufacturing lines/industrial products and support the realization of a smart business model based on Manufacturing as a Service (MaaS). MaaS can lead to more customised manufacturing processes and products, and can also support the refactoring of manufacturing lines in the case of crisis situations, such as a pandemic. 
The technical developments in DIGITbrain are based on the results of the CloudiFacturing project that implemented a cloud-based platform, combined with a digital marketplace as a business gateway, for the execution of simulation or optimisation applications and workflows. However, workflows and applications in CloudiFacturing are typically monolithic, tightly coupling algorithms with models and data sources, making them applicable to only one particular scenario. In order to improve the reusability of various assets (i.e. data, models and algorithms), DIGITbrain clearly separates these assests from each other and enables the creation of DMA (data-model-algorithm) tuples that represent a certain instance of a digital twin. Such digital twin or DMA tuple instances can then be executed as a set of interconnected microservices on the targeted cloud, or even on edge and fog computing resources. 
For the automated deployment and run-time management of microservices-based applications, DIGITbrain utilises the MiCADO cloud to edge orchestration framework that is responsible for deploying the instantiated DMA tuples on central cloud computing resources, or on edge and fog nodes closer to the data sources. However, as the various data, model and algorithm assets are created separately, a specific challenge has emerged to automatically and dynamically generate the deployment descriptors required by MiCADO during the publishing and authoring process. 
MiCADO uses an Application Description Template (ADT) based on the OASIS TOSCA (Topology and Orchestration Specification for Cloud Applications) standard specification to describe the application topology to be deployed and the various policies that govern the application’s run-time behaviour. In all previous application scenarios, the ADT was created in a single manual step by the application developer/owner. However, in DIGITbrain the ADT needs to be programmatically assembled from previously published fragments that represent the individual data, model and algorithm assets, as well as representations of the cloud, fog and edge resources that comprise the infrastructure. 
This presentation will give an overview of how MiCADO is applied within the DIGITbrain platform, what extensions were required to support the targeted application scenarios, and how a MICADO ADT can be dynamically assembled from metadata of individual assets. 
                    </div>                
                    </div>
                </div>
            </td>
        </tr>

        <tr>
            <td>2:15&mdash;2:30</td>
            <td>
                <div class="ui list">
                    <div class="item">
                        <div class="header">Notebook-as-a-VRE (NaaVRE): from private notebooks to a collaborative cloud virtual research environment</div>
                        Zhiming Zhao, Spiros Koulouzis, Riccardo Bianchi, Siamak Farshidi, Ruyue Xin, Yuandou Wang, Na Li, Yifang Shi, Joris Timmermans and W. Daniel Kissling
                    <div class="ui accordion">
                    <div class="title">
                    <i class="dropdown icon"></i>Abstract
                    </div>
                    <div class="content">
                    <p class="transition hidden">
                    Virtual Research Environments (VREs) provide user-centric support in the lifecycle of research activities, e.g., discovering and accessing research assets, composing and executing application workflows. A typical VRE is often implemented as an integrated environment, which includes a catalog of research assets, a workflow management system, a data management framework, and tools for enabling collaboration among users. Notebook environments, such as Jupyter, allow researchers to rapidly prototype scientific code and share their experiments as online accessible notebooks. However, such notebook environments do not have seamless support for running heavy computations on remote infrastructure, finding and accessing software code inside notebooks. This paper investigates the gap between a notebook environment and a VRE and proposes an embedded VRE solution for the Jupyter environment called Notebook-as-a-VRE.
                      
                      </div>                

                    </div>
                </div>
            </td>
        </tr>

        <tr class="warning">
            <td>2:30&mdash;3:00</td>
            <td>
                <div class="ui list">
                    <div class="item">
                        <i class="coffee icon"></i>
                        <div class="content">
                            <div class="header">Tea / Coffee Break</div>
                        </div>
                    </div>
                </div>
            </td>
            <td></td>
        </tr>

    <tr class="active">
        <td colspan="3"><strong>Panel Session</td>
    </tr>
    <tr>
        <td>3:00&mdash;4:30</td>
        <td>
            <div class="ui list">
                <div class="item">
                    <div class="header">Teams of CI Professionals: Recruitment & Retention, Management, Team-building, and Motivation</div>
                   A joint panel session with <a href="http://www.oscer.ou.edu/virtualresidency2021.php">Virtual Residency</a></br>
              Moderator: Sandra Gesing</br>
              Panelists: Ian Cosden, Amy Neeser, Christina Maimone, Scott Hampton
          This is a different Zoom URL than IWSG! </br>
Please go to the website of the <a href="http://www.oscer.ou.edu/virtualresidency2021.php">Virtual Residency</a> for details to register and receive the Zoom URL!
                </div>
            </div>
        </td>
    </tr>


    </tbody>
</table>
-->




